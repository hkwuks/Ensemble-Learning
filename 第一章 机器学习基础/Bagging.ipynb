{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0a8f61be024eba58adef938c9aa1e29e02cb3dece83a5348b1a2dafd16a070453",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Bagging"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Bagging不仅仅集成模型最后的预测结果，同时采用一定策略来影响基模型训练，保证基模型可以服从一定的假设。\n",
    "\n",
    "Bagging的核心在于**自助采样（bootstrap）**这一概念，即有放回的从数据集中进行采样。首先我们随机取出一个样本放入采样集合中，再把这个样本放回初始数据集，重复K次采样，最终我们可以获得一个大小为K的样本集合。同样的方法， 我们可以采样出T个含K个样本的采样集合，然后基于每个采样集合训练出一个基学习器，再将这些基学习器进行结合，这就是Bagging的基本流程。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 举例"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1000, 20) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# 创建一个含有1000个样本20维特征的随机分类数据集\n",
    "from sklearn.datasets import make_classification\n",
    "x,y=make_classification(n_samples=1000,n_features=20,n_informative=15,n_redundant=5,random_state=5)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.855 (0.035)\n"
     ]
    }
   ],
   "source": [
    "from numpy import mean,std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score,RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "x, y = make_classification(n_samples=1000, n_features=20, n_informative=15,n_redundant=5, random_state=5)\n",
    "model=BaggingClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, x, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "source": [
    "Bagging算法（英语：Bootstrapaggregating，引导聚集算法），又称装袋算法，是机器学习领域的一种团体学习算法。最初由Leo Breiman于1996年提出。Bagging算法可与其他分类、回归算法结合，提高其准确率、稳定性的同时，通过降低结果的方差，避免过拟合的发生。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "给定一个大小为n的训练集D，Bagging算法从中均匀、有放回地（即使用自助抽样法）选出m个大小为n'的子集Di，作为新的训练集。在这m个训练集上使用分类、回归等算法，则可得到m个模型，再通过取平均值、取多数票等方法，即可得到Bagging的结果。\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "1.Bagging通过降低基分类器的方差，改善了泛化误差。\n",
    "\n",
    "2.其性能依赖于基分类器的稳定性；如果基分类器不稳定，bagging有助于降低训练数据的随机波动导致的误差；如果稳定，则集成分类器的误差主要由基分类器的偏倚引起。\n",
    "\n",
    "3.由于每个样本被选中的概率相同，因此bagging并不侧重于训练数据集中的任何特定实例。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}